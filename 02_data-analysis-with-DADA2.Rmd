---
title: "Dada2 tutorial"
output: 
  github_document:
    toc: true
    toc_depth: 2
---

#Tutoriel Dada2

##Importation des librairies et préparation des données

```{r}
library("Rcpp")
library("dada2")
```
On importe les galeries nécessaires pour la réalisation de l'excercie

```{r}
path <- "~/MiSeq_SOP"
list.files(path)
```
Chargement des données fastq sur la donnée path

```{r}
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```
On sépare les forward et les reverse. Les forward seront associés à fnFS et les reverse à fnRs.
Association des fnFs et fnRS entre eux (correspondance entre les données)

```{r}
plotQualityProfile(fnFs[1:4])
```
Affichage du graphe de qualité des données fnFs.
En gris : heat map de la fréquence de chaque score de qualité de chaque position de base
En vert : score de qualité moyen de chaque base
En rouge : proportion graduée des reads jusqu'à une certaine position (ici, elle est plate car les reads faites sur Illumina sont de même longueur donc ce n'est pas très utile pour ce cas présent)
Résultat : bon. On va couper les derniers nucléotides car on observe une chute sur la partie droite du graphe. Cette coupure de nucléotide permettra de donner des résultats plus fiable pour la suite des opérations. Ici, on va tronquer les nucléotides à partir de la position 240 (on retire les 10 derniers nucléotides).

```{r}
plotQualityProfile(fnRs[1:4])
```
Affichage du graphe de qualité des données fnRs
Même codes couleurs que fnFs
Résultat : les reads reverse sont de moins bonne qualité en particulier vers la fin. Ce n'est pas grave mais les nucléotides à partir de la position 160 seront supprimés (soit 90 nucléotides)

```{r}
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```
Les données fnFs et fnRs vont être filtrées. Tout d'abord, elles vont être placées sous un nouveau nom (ici, filtFs et filtRs)

```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE)
head(out)
```
Les données sont filtrés, ie que les derniers nucléotides vont être retirés (fnFs : à partir de 240; fnRd : à partir de 160). Seulement 2 erreurs peuvent être attendues lors de la filtration

##Calcul des erreurs et visualisation de ces derniers

calcul modèle erreurs à partir des données de séquençage. Applique méthodes sur forward puis reverse
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
```
Estimation du taux d'erreur de filtFs

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)
```
Estimation du taux d'erreur de filtRs

```{r}
plotErrors(errF, nominalQ=TRUE)
```
Représentation des fréquences d'erreurs estimées.
Points gris : taux d'erreurs observées pour chacun des scores de qualité consensus.
Ligne noire : taux d'erreurs estimé après que l'algorithme ait réuni toutes les informations (liée aux taux d'erreurs estimés)
Ligne rouge : taux d'erreurs attendu selon le Q-score
Corrélation entre les taux d'erreurs estimés et observés, diminution quand le score de qualité augmente -> on peut poursuivre.

##Application de Dada2 aux données

```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```
Application de l'algorithme d'inférence aux données filtrées et ajustées des reads forward

```{r}
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```
Application de l'algorithme d'inférence aux données filtrées et ajustées des reads reverse

```{r}
dadaFs[[1]]
```
Dada2 a déduit 128 séquences variantes à partir des 1979 séquences uniques.

##Alignement des séquences Forward et Reverse

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
head(mergers[[1]])
```
Alignement des séquences pour former des contigues. L'alignement se fait avec les séquences forward et reverse se correspondant sur au moins 12 bases.

## Table d'observation

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```
Construction d'ASV (table d'OTU)? Ici, sur les 20 échantillons, 293 ASV ont été crée

```{r}
table(nchar(getSequences(seqtab)))
```
Visualisation de la distribution des longueurs de séquences. Ici, les différentes séquences obtenues sont d'une longueur située dans la région V4 de l'ARN16S

##Elimination des séquences chimériques

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
Dada a corrigé les erreurs de substitutions et d'index. Mais il reste les séquences chimériques (séquence contenant de l'information de plusiseurs autres séquences) à retirer. L'identification se fait par construction de deux séquences (une gauche et une droite), provenant de séquences dites parents retrouvées abondament dans notre échantillon. Ici, 20232 séquences chimériques ont été identifiées

```{r}
sum(seqtab.nochim)/sum(seqtab)
```
Ces séquences chimériques représentent 3.5% de notre jeu de donnée

#table récap, résumé des filtres effectués

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
Récapitulatif des reads qui ont traversé les différentes étapes de filtration et de tri.

#télécharger base de silva

```{bash}
wget https://zenodo.org/record/3986799/files/silva_nr99_v138_train_set.fa.gz
```

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/Dada2_Phyloseq_CC1/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```
Assignation des séquences étudiées à la classification. Cette assignation se fait par rapport à une base de séquence de référence ayant une classification connue.

Partie non réalisé
#télécharger assignement espèces silva

```{bash}
wget https://zenodo.org/record/3986799/files/silva_species_assignment_v138.fa.gz
```

```{r}
taxa <- addSpecies(taxa, "~/Dada2_Phyloseq_CC1/silva_species_assignment_v138.fa.gz")
```
Fin de la partie no réalisé


```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

```{bash}
wget http://www2.decipher.codes/DECIPHER_2.18.1.zip
```

```{bash}
wget http://www2.decipher.codes/Classification/TrainingSets/SILVA_SSU_r138_2019.RData
```

```{r}
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
```

```{r}
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")
```

#essai phyloseq

```{r}
samples.out <- rownames(seqtab.nochim)
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
gender <- substr(subject,1,1)
subject <- substr(subject,2,999)
day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
samdf <- data.frame(Subject=subject, Gender=gender, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- samples.out
```

```{r}
library("phyloseq")
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps <- prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample
```

```{r}
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
```

```{r}
plot_richness(ps, x="Day", measures=c("Shannon", "Simpson"), color="When")
```

```{r}
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
```

```{r}
plot_ordination(ps.prop, ord.nmds.bray, color="When", title="Bray NMDS")
```

```{r}
library("Biostrings")
library("ggplot2")
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Day", fill="Family") + facet_wrap(~When, scales="free_x")
```

